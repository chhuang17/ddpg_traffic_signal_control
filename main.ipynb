{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1964daa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.nn.functional import normalize\n",
    "from vissim import Vissim\n",
    "from buffer import ReplayBuffer\n",
    "from ddpg import DDPG\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455867c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "alpha = 0.0001\n",
    "beta = 0.0003\n",
    "gamma = 0.6\n",
    "tau = 0.005\n",
    "batch_size = 32\n",
    "buffer_size = 90000\n",
    "n_agents = 1\n",
    "nodes = 3\n",
    "\n",
    "# std for exploration\n",
    "# decay rate = 1 means constant std.\n",
    "init_std = 1\n",
    "min_std = 0.05\n",
    "decay_rate = 0.99995\n",
    "\n",
    "evaluate = False\n",
    "second_stage = False\n",
    "\n",
    "# simulation parameters\n",
    "decision_point = [i for i in range(900, 4501, 60)]\n",
    "sim_time = 4500\n",
    "num_episodes = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689a0106",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 150\n",
    "ddpg_agent = DDPG(alpha=alpha, beta=beta, gamma=gamma, tau=tau, chkpt_dir='/Users/chhuang/ddpg_model/model/')\n",
    "memory = ReplayBuffer(max_size=buffer_size, batch_size=batch_size, n_agents=n_agents)\n",
    "\n",
    "Path = os.getcwd()\n",
    "Filename = os.path.join(Path, r\"C:\\Users\\chhuang\\ddpg_model\\vissim_network\\multi-3.inpx\")\n",
    "env = Vissim(Filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef80c811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_transform(car_D, scooter_D, car_S, scooter_S):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    car_D = torch.tensor(car_D, dtype=torch.float).to(device)\n",
    "    car_D = normalize(car_D, dim=0)\n",
    "    car_D = torch.stack([car_D])\n",
    "\n",
    "    scooter_D = torch.tensor(scooter_D, dtype=torch.float).to(device)\n",
    "    scooter_D = normalize(scooter_D, dim=0)\n",
    "    scooter_D = torch.stack([scooter_D])\n",
    "\n",
    "    car_S = torch.tensor(car_S, dtype=torch.float).to(device)\n",
    "    car_S = normalize(car_S, dim=0)\n",
    "    car_S = torch.stack([car_S])\n",
    "\n",
    "    scooter_S = torch.tensor(scooter_S, dtype=torch.float).to(device)\n",
    "    scooter_S = normalize(scooter_S, dim=0)\n",
    "    scooter_S = torch.stack([scooter_S])\n",
    "    \n",
    "    return car_D, scooter_D, car_S, scooter_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618c57dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 儲存每一個episode獲得的平均累積獎勵\n",
    "avg_rewards_per_episode = []\n",
    "avg_global_rewards_per_episode = []\n",
    "\n",
    "# 儲存每一個episode的平均時相時間\n",
    "avg_phase_time_1_per_episode = []\n",
    "avg_phase_time_2_per_episode = []\n",
    "\n",
    "# 儲存每一個episode的平均時差\n",
    "avg_offset_per_episode = []\n",
    "\n",
    "# 儲存每一個episode的平均loss\n",
    "avg_actor_loss_per_episode = []\n",
    "avg_critic_loss_per_episode = []\n",
    "\n",
    "# 儲存每筆資料的loss\n",
    "all_critic_loss = []\n",
    "all_actor_loss = []\n",
    "\n",
    "# 儲存旅行時間資料\n",
    "avg_travtime_car_per_episode = np.zeros((num_episodes, 36))\n",
    "avg_travtime_scooter_per_episode = np.zeros((num_episodes, 36))\n",
    "\n",
    "# 儲存每回合花費時間\n",
    "spent_times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b89d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluate == True or second_stage == True:\n",
    "    ddpg_agent.load_checkpoint()\n",
    "\n",
    "np.set_printoptions(precision=4, threshold=10000)\n",
    "total_step = 0\n",
    "\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    start = time.time()\n",
    "    print('******** start episode %s ********' % episode)\n",
    "    env.stop_simulation()\n",
    "    env.del_pre_simulation()\n",
    "    env.set_randseed(episode + 10000)\n",
    "    env.reset()\n",
    "    env.set_signal_program(3)\n",
    "    \n",
    "    # 創建一個回合的reward table, avg. reward table, ...\n",
    "    rewards = np.zeros(n_agents)\n",
    "    avg_rewards = np.zeros(n_agents)\n",
    "    \n",
    "    phase_1 = np.zeros(nodes)\n",
    "    phase_2 = np.zeros(nodes)\n",
    "    avg_phase_1 = np.zeros(nodes)\n",
    "    avg_phase_2 = np.zeros(nodes)\n",
    "    \n",
    "    offset = np.zeros(nodes)\n",
    "    avg_offset = np.zeros(nodes)\n",
    "    \n",
    "    actor_loss = np.zeros(n_agents)\n",
    "    critic_loss = np.zeros(n_agents)\n",
    "    avg_actor_loss = np.zeros(n_agents)\n",
    "    avg_critic_loss = np.zeros(n_agents)\n",
    "    \n",
    "    # 熱機840s\n",
    "    env.quickmode(1)\n",
    "    env.warm_up(840)\n",
    "    env.break_time(900)\n",
    "    \n",
    "    # 取得900s的狀態\n",
    "    car_D, scooter_D, car_S, scooter_S = env.get_all_states()\n",
    "    car_D, scooter_D, car_S, scooter_S = input_transform(car_D, scooter_D, car_S, scooter_S)\n",
    "\n",
    "    samples = 0\n",
    "    \n",
    "    while True:\n",
    "        if env.time in decision_point:\n",
    "            if total_step >= 60 * 500:\n",
    "                std *= decay_rate\n",
    "                std = max(min_std, std)\n",
    "            else:\n",
    "                std = init_std\n",
    "            # get actions\n",
    "            print('std: %.4f' % std)\n",
    "            splits, first_greens = ddpg_agent.choose_actions(car_D, scooter_D, car_S, scooter_S, std)\n",
    "            splits_for_vissim = splits.squeeze(0).cpu().numpy()\n",
    "            first_greens_for_vissim = first_greens.squeeze(0).cpu().numpy()\n",
    "            \n",
    "            # 更新時制計畫\n",
    "            env.update_timing_plans(splits_for_vissim, first_greens_for_vissim)\n",
    "            \n",
    "        # 輸出並記錄新的時制計畫\n",
    "        new_timing_plans = env.get_timing_plans()\n",
    "        updated_offsets = env.get_offsets()\n",
    "        print('new_timing_plans:', new_timing_plans)\n",
    "        print('updated_offsets:', updated_offsets)\n",
    "        for i in range(nodes):\n",
    "            phase_1[i] += new_timing_plans['%s' % (i+1)][0]\n",
    "            phase_2[i] += new_timing_plans['%s' % (i+1)][1]\n",
    "            offset[i] += updated_offsets[i]\n",
    "        \n",
    "        # 執行新的時制計畫\n",
    "        env.execute_new_timing_plans(env.time)\n",
    "\n",
    "        # 取得新狀態資訊\n",
    "        car_D_, scooter_D_, car_S_, scooter_S_ = env.get_all_states()\n",
    "        car_D_, scooter_D_, car_S_, scooter_S_ = input_transform(car_D_, scooter_D_, car_S_, scooter_S_)\n",
    "      \n",
    "        # ========================================== 以下計算reward ==========================================\n",
    "        # 創建單次動作的reward table\n",
    "        reward = np.zeros(n_agents)\n",
    "        \n",
    "        # 從VISSIM取得資料\n",
    "        throughput_car, throughput_scooter = env.get_total_throughput()\n",
    "        queue_length = env.get_total_queue_length()\n",
    "        \n",
    "        for i in range(n_agents):\n",
    "            # 這邊要計算每個路口獲得的reward\n",
    "            # 考慮項目：路口通過車輛數、等候車隊長度\n",
    "            reward[i] = throughput_car[i] + 0.3 * throughput_scooter[i] - 0.44 * queue_length[i]\n",
    "            rewards[i] += reward[i]\n",
    "        \n",
    "        print('reward:', reward)\n",
    "\n",
    "        memory.store_transition(car_D, scooter_D, car_S, scooter_S, splits, first_greens, reward,\n",
    "                                car_D_, scooter_D_, car_S_, scooter_S_)\n",
    "        samples += 1\n",
    "        total_step += 1\n",
    "        print('###### have got %s sample(s) ######' % samples)\n",
    "\n",
    "        if memory.ready() == True:\n",
    "            print('learning ...')\n",
    "            c_loss, a_loss = ddpg_agent.learn(memory)\n",
    "            all_critic_loss.append(c_loss)\n",
    "            all_actor_loss.append(a_loss)\n",
    "            for i in range(n_agents):\n",
    "                critic_loss[i] += c_loss[i]\n",
    "                actor_loss[i] += a_loss[i]\n",
    "        \n",
    "        if env.time >= sim_time:\n",
    "            break\n",
    "        else:\n",
    "            car_D = car_D_\n",
    "            scooter_D = scooter_D_\n",
    "            car_S = car_S_\n",
    "            scooter_S = scooter_S_\n",
    "        \n",
    "    # 計算一回合的 avg. reward, avg. phase time, ...\n",
    "    for i in range(n_agents):\n",
    "        avg_rewards[i] += rewards[i] / 60\n",
    "        if episode == 1:\n",
    "            avg_critic_loss[i] += critic_loss[i] / 29\n",
    "            avg_actor_loss[i] += actor_loss[i] / 29\n",
    "        else:\n",
    "            avg_critic_loss[i] += critic_loss[i] / 60\n",
    "            avg_actor_loss[i] += actor_loss[i] / 60\n",
    "            \n",
    "    for i in range(nodes):\n",
    "        avg_phase_1[i] += phase_1[i] / 60\n",
    "        avg_phase_2[i] += phase_2[i] / 60\n",
    "        avg_offset[i] += offset[i] / 60\n",
    "        \n",
    "    avg_rewards_per_episode.append(avg_rewards)\n",
    "    avg_global_rewards_per_episode.append(float(avg_rewards))\n",
    "    avg_phase_time_1_per_episode.append(avg_phase_1)\n",
    "    avg_phase_time_2_per_episode.append(avg_phase_2)\n",
    "    avg_offset_per_episode.append(avg_offset)\n",
    "    avg_critic_loss_per_episode.append(avg_critic_loss)\n",
    "    avg_actor_loss_per_episode.append(avg_actor_loss)\n",
    "    \n",
    "    # 儲存旅行時間資料\n",
    "    travtime_car, travtime_scooter = env.get_travel_time()\n",
    "    avg_travtime_car_per_episode[episode - 1] += travtime_car\n",
    "    avg_travtime_scooter_per_episode[episode - 1] += travtime_scooter\n",
    "        \n",
    "    end = time.time()\n",
    "    spent_time = end - start\n",
    "    spent_times.append(spent_time)\n",
    "    print('avg. rewards in this episode:', avg_rewards)\n",
    "    print('avg. critic loss in this episode:', avg_critic_loss)\n",
    "    print('******** end episode %s ********' % episode)\n",
    "    print('spent %.2f (s) for episode %s' % (spent_time, episode))\n",
    "    \n",
    "    if evaluate == False:\n",
    "        ddpg_agent.save_checkpoint()  # 每回合都存\n",
    "    \n",
    "    episode += 1\n",
    "\n",
    "\n",
    "# ========================================== 記錄訓練參數及回合 ==========================================\n",
    "max_time_per_episode = max(spent_times)\n",
    "min_time_per_episode = min(spent_times)\n",
    "avg_time_per_episode = sum(spent_times) / len(spent_times)\n",
    "print('====== training %s episodes ======' % len(spent_times))\n",
    "print('max_time_per_episode: %.2f s' % max_time_per_episode)\n",
    "print('min_time_per_episode: %.2f s' % min_time_per_episode)\n",
    "print('avg_time_per_episode: %.2f s' % avg_time_per_episode)\n",
    "\n",
    "with open('model/log.txt', 'w') as f:\n",
    "    f.write('====== training %s episodes ======' % len(spent_times))\n",
    "    f.write('\\nmax_time_per_episode: %.2f s' % max_time_per_episode)\n",
    "    f.write('\\nmin_time_per_episode: %.2f s' % min_time_per_episode)\n",
    "    f.write('\\navg_time_per_episode: %.2f s' % avg_time_per_episode)\n",
    "    \n",
    "    f.write('\\nalpha: %s' % alpha)\n",
    "    f.write('\\nbeta: %s' % beta)\n",
    "    f.write('\\ngamma: %s' % gamma)\n",
    "    f.write('\\ntau: %s' % tau)\n",
    "    f.write('\\nbatch size: %d' % batch_size)\n",
    "    f.write('\\nbuffer size: %d' % buffer_size)\n",
    "    f.write('\\ninit_std: %s' % init_std)\n",
    "    f.write('\\nmin_std: %s' % min_std)\n",
    "    f.write('\\ndecay rate: %s' % decay_rate)\n",
    "    f.write('\\nnetwork: network')\n",
    "    f.write('\\nnote: ')\n",
    "\n",
    "\n",
    "# ========================================== 以下記錄訓練結果 ==========================================\n",
    "global_agent = []\n",
    "\n",
    "for i in range(len(avg_rewards_per_episode)):\n",
    "    global_agent.append(avg_rewards_per_episode[i][0])\n",
    "\n",
    "# ========================================== 將reward寫入txt檔 ==========================================\n",
    "with open('model/results/reward.txt', 'w') as f:\n",
    "    f.write('[')\n",
    "    for i in range(len(global_agent)):\n",
    "        if i == len(global_agent) - 1:\n",
    "            f.write('%s' % (global_agent[i]) + ']')\n",
    "        else:\n",
    "            f.write('%s' % (global_agent[i]) + ',')\n",
    "\n",
    "with open('model/results/global_reward.txt', 'w') as f:\n",
    "    f.write('[')\n",
    "    for i in range(len(avg_global_rewards_per_episode)):\n",
    "        if i == len(avg_global_rewards_per_episode) - 1:\n",
    "            f.write('%s' % (avg_global_rewards_per_episode[i]) + ']')\n",
    "        else:\n",
    "            f.write('%s' % (avg_global_rewards_per_episode[i]) + ',')\n",
    "            \n",
    "\n",
    "# data preprocessing\n",
    "travtime_car_for_each_detectors = [[] for i in range(36)]\n",
    "travtime_scooter_for_each_detectors = [[] for i in range(36)]\n",
    "\"\"\"\n",
    "0-11: 111, 112, 113, ... , 141, 142, 143\n",
    "12-23: 211, 212, 213, ... , 241, 242, 243\n",
    "24-35: 311, 312, 313, ... , 341, 342, 343\n",
    "\n",
    "\"\"\"\n",
    "for i in range(num_episodes):\n",
    "    for j in range(36):\n",
    "        travtime_car_for_each_detectors[j].append(avg_travtime_car_per_episode[i][j])\n",
    "        travtime_scooter_for_each_detectors[j].append(avg_travtime_scooter_per_episode[i][j])\n",
    "\n",
    "# ========================================== 將旅行時間寫入txt檔 ==========================================\n",
    "for i in range(36):\n",
    "    with open('model/results/traveltime/car travel time_%s.txt' % (i+1), 'w') as f:\n",
    "        f.write('[')\n",
    "        for j in range(len(travtime_car_for_each_detectors[i])):\n",
    "            if j == len(travtime_car_for_each_detectors[i]) - 1:\n",
    "                f.write('%s' % (travtime_car_for_each_detectors[i][j]) + ']')\n",
    "            else:\n",
    "                f.write('%s' % (travtime_car_for_each_detectors[i][j]) + ',')\n",
    "                \n",
    "for i in range(36):\n",
    "    with open('model/results/traveltime/scooter travel time_%s.txt' % (i+1), 'w') as f:\n",
    "        f.write('[')\n",
    "        for j in range(len(travtime_scooter_for_each_detectors[i])):\n",
    "            if j == len(travtime_scooter_for_each_detectors[i]) - 1:\n",
    "                f.write('%s' % (travtime_scooter_for_each_detectors[i][j]) + ']')\n",
    "            else:\n",
    "                f.write('%s' % (travtime_scooter_for_each_detectors[i][j]) + ',')\n",
    "\n",
    "# ========================================== 將loss寫入txt檔 ==========================================\n",
    "global_agent_critic_loss = []\n",
    "global_agent_actor_loss = []\n",
    "\n",
    "for i in range(len(avg_critic_loss_per_episode)):\n",
    "    global_agent_critic_loss.append(float(avg_critic_loss_per_episode[i][0]))\n",
    "    global_agent_actor_loss.append(float(avg_actor_loss_per_episode[i][0]))\n",
    "\n",
    "with open('model/results/loss/global_agent_critic_loss.txt', 'w') as f:\n",
    "    f.write('[')\n",
    "    for i in range(len(global_agent_critic_loss)):\n",
    "        if i == len(global_agent_critic_loss) - 1:\n",
    "            f.write('%s' % (global_agent_critic_loss[i]) + ']')\n",
    "        else:\n",
    "            f.write('%s' % (global_agent_critic_loss[i]) + ',')\n",
    "\n",
    "with open('model/results/loss/global_agent_actor_loss.txt', 'w') as f:\n",
    "    f.write('[')\n",
    "    for i in range(len(global_agent_actor_loss)):\n",
    "        if i == len(global_agent_actor_loss) - 1:\n",
    "            f.write('%s' % (global_agent_actor_loss[i]) + ']')\n",
    "        else:\n",
    "            f.write('%s' % (global_agent_actor_loss[i]) + ',')\n",
    "\n",
    "# ========================================== 將時相變化寫入txt檔 ==========================================\n",
    "node_1_phase_1 = []\n",
    "node_2_phase_1 = []\n",
    "node_3_phase_1 = []\n",
    "\n",
    "node_1_phase_2 = []\n",
    "node_2_phase_2 = []\n",
    "node_3_phase_2 = []\n",
    "\n",
    "for i in range(len(avg_phase_time_1_per_episode)):\n",
    "    node_1_phase_1.append(avg_phase_time_1_per_episode[i][0])\n",
    "    node_2_phase_1.append(avg_phase_time_1_per_episode[i][1])\n",
    "    node_3_phase_1.append(avg_phase_time_1_per_episode[i][2])\n",
    "\n",
    "for i in range(len(avg_phase_time_2_per_episode)):\n",
    "    node_1_phase_2.append(avg_phase_time_2_per_episode[i][0])\n",
    "    node_2_phase_2.append(avg_phase_time_2_per_episode[i][1])\n",
    "    node_3_phase_2.append(avg_phase_time_2_per_episode[i][2])\n",
    "\n",
    "with open('model/results/phase/node_1_phase_1.txt', 'w') as f:\n",
    "    f.write('[')\n",
    "    for i in range(len(node_1_phase_1)):\n",
    "        if i == len(node_1_phase_1) - 1:\n",
    "            f.write('%s' % (node_1_phase_1[i]) + ']')\n",
    "        else:\n",
    "            f.write('%s' % (node_1_phase_1[i]) + ',')\n",
    "\n",
    "with open('model/results/phase/node_1_phase_2.txt', 'w') as f:\n",
    "    f.write('[')\n",
    "    for i in range(len(node_1_phase_2)):\n",
    "        if i == len(node_1_phase_2) - 1:\n",
    "            f.write('%s' % (node_1_phase_2[i]) + ']')\n",
    "        else:\n",
    "            f.write('%s' % (node_1_phase_2[i]) + ',')\n",
    "\n",
    "with open('model/results/phase/node_2_phase_1.txt', 'w') as f:\n",
    "    f.write('[')\n",
    "    for i in range(len(node_2_phase_1)):\n",
    "        if i == len(node_2_phase_1) - 1:\n",
    "            f.write('%s' % (node_2_phase_1[i]) + ']')\n",
    "        else:\n",
    "            f.write('%s' % (node_2_phase_1[i]) + ',')\n",
    "\n",
    "with open('model/results/phase/node_2_phase_2.txt', 'w') as f:\n",
    "    f.write('[')\n",
    "    for i in range(len(node_2_phase_2)):\n",
    "        if i == len(node_2_phase_2) - 1:\n",
    "            f.write('%s' % (node_2_phase_2[i]) + ']')\n",
    "        else:\n",
    "            f.write('%s' % (node_2_phase_2[i]) + ',')\n",
    "\n",
    "with open('model/results/phase/node_3_phase_1.txt', 'w') as f:\n",
    "    f.write('[')\n",
    "    for i in range(len(node_3_phase_1)):\n",
    "        if i == len(node_3_phase_1) - 1:\n",
    "            f.write('%s' % (node_3_phase_1[i]) + ']')\n",
    "        else:\n",
    "            f.write('%s' % (node_3_phase_1[i]) + ',')\n",
    "\n",
    "with open('model/results/phase/node_3_phase_2.txt', 'w') as f:\n",
    "    f.write('[')\n",
    "    for i in range(len(node_3_phase_2)):\n",
    "        if i == len(node_3_phase_2) - 1:\n",
    "            f.write('%s' % (node_3_phase_2[i]) + ']')\n",
    "        else:\n",
    "            f.write('%s' % (node_3_phase_2[i]) + ',')\n",
    "            \n",
    "            \n",
    "# ========================================== 將時差變化寫入txt檔 ==========================================\n",
    "node_1_offset = []\n",
    "node_2_offset = []\n",
    "node_3_offset = []\n",
    "\n",
    "for i in range(len(avg_offset_per_episode)):\n",
    "    node_1_offset.append(avg_offset_per_episode[i][0])\n",
    "    node_2_offset.append(avg_offset_per_episode[i][1])\n",
    "    node_3_offset.append(avg_offset_per_episode[i][2])\n",
    "\n",
    "with open('model/results/phase/node_1_offset.txt', 'w') as f:\n",
    "    f.write('[')\n",
    "    for i in range(len(node_1_offset)):\n",
    "        if i == len(node_1_offset) - 1:\n",
    "            f.write('%s' % (node_1_offset[i]) + ']')\n",
    "        else:\n",
    "            f.write('%s' % (node_1_offset[i]) + ',')\n",
    "\n",
    "with open('model/results/phase/node_2_offset.txt', 'w') as f:\n",
    "    f.write('[')\n",
    "    for i in range(len(node_2_offset)):\n",
    "        if i == len(node_2_offset) - 1:\n",
    "            f.write('%s' % (node_2_offset[i]) + ']')\n",
    "        else:\n",
    "            f.write('%s' % (node_2_offset[i]) + ',')\n",
    "\n",
    "with open('model/results/phase/node_3_offset.txt', 'w') as f:\n",
    "    f.write('[')\n",
    "    for i in range(len(node_3_offset)):\n",
    "        if i == len(node_3_offset) - 1:\n",
    "            f.write('%s' % (node_3_offset[i]) + ']')\n",
    "        else:\n",
    "            f.write('%s' % (node_3_offset[i]) + ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.stop_simulation()\n",
    "env.del_pre_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81351b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
